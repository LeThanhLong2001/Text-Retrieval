{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x62qdTKS9DCA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "qlS1zmFZ2Gjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and read dataset"
      ],
      "metadata": {
        "id": "x62qdTKS9DCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# link: https://drive.google.com/file/d/1ydGNBdRVloX9rtxsKrMSnUNFG43Qv1sl/view?usp=sharing\n",
        "!gdown --id 1ydGNBdRVloX9rtxsKrMSnUNFG43Qv1sl\n",
        "!unzip news_corpus.zip"
      ],
      "metadata": {
        "id": "Rjn6gbXV9CWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define normalize text function and create vocab"
      ],
      "metadata": {
        "id": "E4TmYC6oFoGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download vietnamese stopwords: https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt\n",
        "!gdown --id 1W9zVRz--bHlbBXbCSmoWHBO_2Cs4EhPY\n",
        "!unzip vn_stopwords.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21LiNGAqoY0r",
        "outputId": "486da6e8-c60b-4668-fefa-cf9bea616c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1W9zVRz--bHlbBXbCSmoWHBO_2Cs4EhPY\n",
            "To: /content/vn_stopwords.zip\n",
            "100% 6.89k/6.89k [00:00<00:00, 9.77MB/s]\n",
            "Archive:  vn_stopwords.zip\n",
            "  inflating: vietnamese-stopwords.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def remove_punctuations(text: str) -> str:\n",
        "  return re.sub(r'[^\\w\\sàáãạảăắằẳẵặâấầẩẫậèéẹẻẽêềếểễệđìíĩỉịòóõọỏôốồổỗộơớờởỡợùúũụủưứừửữựỳỵỷỹýÀÁÃẠẢĂẮẰẲẴẶÂẤẦẨẪẬÈÉẸẺẼÊỀẾỂỄỆĐÌÍĨỈỊÒÓÕỌỎÔỐỒỔỖỘƠỚỜỞỠỢÙÚŨỤỦƯỨỪỬỮỰỲỴỶỸÝ]', ' ', text)\n",
        "\n",
        "def remove_email(text: str) -> str:\n",
        "  return re.sub(r'\\S*@\\S*\\s?', '', text)\n",
        "\n",
        "def remove_url(text: str) -> str:\n",
        "  return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "with open('vietnamese-stopwords.txt', 'r', encoding='utf8') as f:\n",
        "  vn_stopwords = f.read().splitlines()\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "  new_text = text\n",
        "  for w in vn_stopwords:\n",
        "    new_text = re.sub(f'\\s{w}\\s', ' ', new_text)\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "  normalized_text = text.lower()\n",
        "  normalized_text = unicodedata.normalize('NFKC', normalized_text)\n",
        "  normalized_text = remove_email(normalized_text)\n",
        "  normalized_text = remove_url(normalized_text)\n",
        "  normalized_text = remove_punctuations(normalized_text)\n",
        "  normalized_text = remove_stopwords(normalized_text)\n",
        "\n",
        "  return normalized_text"
      ],
      "metadata": {
        "id": "v-O_-9xCGVtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create similiarity measurement function using cosine similarity\n",
        "\n",
        "$cosine\\_similarity(a, b) = \\frac{a ⋅ b}{|a||b|} = \\frac{\\sum_{i = 1}^{N}a_ib_i}{\\sqrt{\\sum_{i = 1}^{N}a_i^2}\\sqrt{\\sum_{i = 1}^{N}b_i^2}}$"
      ],
      "metadata": {
        "id": "1CXZUi10HpVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(a: np.ndarray, b: np.ndarray) -> np.float64:\n",
        "  numerator = np.dot(a, b)\n",
        "  denominator = np.linalg.norm(a) * np.linalg.norm(b)\n",
        "\n",
        "  return numerator / denominator"
      ],
      "metadata": {
        "id": "Gsu29WVBHsnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create vectorize function using bag-of-words on a provided vocab"
      ],
      "metadata": {
        "id": "cuJVtq3eYbEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(text: str, vocab: list) -> np.ndarray:\n",
        "  normalized_text = normalize_text(text)\n",
        "  vec = []\n",
        "  for word in vocab:\n",
        "    vec.append(normalized_text.count(word))\n",
        "\n",
        "  return np.array(vec)"
      ],
      "metadata": {
        "id": "q4J5KyuNYeoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Building Text Retrieval system using Vector Space Model\n",
        "\n"
      ],
      "metadata": {
        "id": "jNV1g8DTzLM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Create vocab"
      ],
      "metadata": {
        "id": "k6Y6h6VbYCr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lists = []\n",
        "vocab = []\n",
        "dataset_root_path = 'news_corpus'\n",
        "filenames = os.listdir(dataset_root_path)\n",
        "for i in tqdm(range(len(filenames) // 200)):\n",
        "  filename = filenames[i]\n",
        "  filepath = os.path.join(dataset_root_path, filename)\n",
        "  with open(filepath, 'r', encoding='utf8') as f:\n",
        "    lines = list(filter(None, f.read().splitlines()))\n",
        "    title = unicodedata.normalize('NFKC', lines[0].strip())\n",
        "    article = ' '.join(lines[1:]).strip()\n",
        "    article = normalize_text(article)\n",
        "    if article == '':\n",
        "      continue\n",
        "    else:\n",
        "      if (title, article) not in doc_lists:\n",
        "        doc_lists.append((title, article))\n",
        "      tokens = list(filter(None, article.split(' ')))\n",
        "      for token in tokens:\n",
        "        if token not in vocab:\n",
        "          vocab.append(token)"
      ],
      "metadata": {
        "id": "cdTVWBlTTrB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d86330-a05a-4e79-de9f-11a7acb714d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 922/922 [03:12<00:00,  4.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Vocab size: {len(vocab)}')\n",
        "print(f'Number of docs: {len(doc_lists)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj_yJaePGhs_",
        "outputId": "de25721b-7053-45ce-e85f-61b425ad3e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 9368\n",
            "Number of docs: 855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Create document-term matrix"
      ],
      "metadata": {
        "id": "iFJEDc-Mp8Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_term_matrix = {}\n",
        "for (title, article) in tqdm(doc_lists):\n",
        "  vec = vectorize(article, vocab)\n",
        "  doc_term_matrix[(title, article)] = vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehkhHfahYU56",
        "outputId": "c65ced05-5c67-4c2d-d7a0-06421b6ffb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 855/855 [02:51<00:00,  4.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Ranking"
      ],
      "metadata": {
        "id": "z1OXdx2KL2D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ranking(query: str, doc_term_matrix: dict, print_top_10: bool = True) -> list:\n",
        "  query_vec = vectorize(query, vocab)\n",
        "  rankings = []\n",
        "  i = 1\n",
        "  for doc_info, vec in doc_term_matrix.items():\n",
        "    score = distance(query_vec, vec)\n",
        "    rankings.append((score, (doc_info[0])))\n",
        "    i += 1\n",
        "  rankings.sort(reverse=True)\n",
        "\n",
        "  if print_top_10 == True:\n",
        "    for rank in rankings[:10]:\n",
        "      print(rank)\n",
        "\n",
        "  return rankings"
      ],
      "metadata": {
        "id": "u_L-rw2rbT_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"điểm thi đại học\"\n",
        "rankings = ranking(query, doc_term_matrix, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muo2FftxdowY",
        "outputId": "592113d9-2674-402f-bd8e-6cd8542df84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.6670716806549295, 'Đảm bảo cơ sở vật chất trước ngày thi tốt nghiệp')\n",
            "(0.6546724335902323, \"Sẽ xử lý nghiêm vụ nữ sinh dùng thiết bị điện tử 'tuồn' đề thi tốt nghiệp THPT\")\n",
            "(0.6388767231972037, 'Cả nước có 38 thí sinh là F0 dự thi tốt nghiệp THPT năm 2022')\n",
            "(0.6342339280242492, 'Bộ trưởng Nguyễn Kim Sơn thị sát thi tốt nghiệp THPT tại Huế | Báo Dân trí')\n",
            "(0.6315782475591162, 'Đề thi môn Giáo dục công dân kỳ thi tốt nghiệp THPT 2022 | VTV.VN')\n",
            "(0.6314463180720822, 'Vì sao Lệnh Hồ Xung có Độc Cô Cửu Kiếm vẫn bại trước Đông Phương Bất Bại?')\n",
            "(0.6284002689891311, 'U19 Việt Nam 4-1 U19 Philippines (H2): Cú sút panenka tuyệt vời')\n",
            "(0.6281683710970668, 'Bất chấp bão giá và nợ nần, giới trẻ chi tiền nhiều hơn để đi chơi: Mua nhà, mua xe đã hết \"vui\"?')\n",
            "(0.6056558126751928, \"Will Smith: 'Chris Rock chưa sẵn sàng nói chuyện với tôi'\")\n",
            "(0.6029601607415258, \"Đại diện Việt Nam phối hợp đăng cai tổ chức cuộc thi tranh biện quốc tế World Scholar's Cup\")\n"
          ]
        }
      ]
    }
  ]
}